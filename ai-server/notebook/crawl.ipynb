{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset for filtering: Idea Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install requests Pillow beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "response = requests.get(\"https://ideasai.com/data-startup-ideas\")\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "ideas = soup.findAll('h3', class_='idea')\n",
    "idea_descriptions = [desc.text for desc in ideas]\n",
    "print(idea_descriptions, len(idea_descriptions))\n",
    "\n",
    "votes = soup.findAll('span', class_='votes')\n",
    "voting = [v.text for v in votes]\n",
    "print(voting)\n",
    "\n",
    "print (len(voting), len(idea_descriptions))\n",
    "\n",
    "import csv\n",
    "\n",
    "file_path='data/valid_ideas.csv'\n",
    "print(os.getcwd())\n",
    "\n",
    "  # Try to open non-existing file to write data\n",
    "with open(file_path, \"x+\") as file:\n",
    "  # Read the contents of the file\n",
    "  for idx in range(len(voting)):\n",
    "    desc = idea_descriptions[idx].replace('\"', '')\n",
    "    vote = voting[idx].replace(',', '')\n",
    "    sen = ';'.join([desc, '1', vote, '\\n'])\n",
    "    file.write(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "response = requests.get(\"https://randomwordgenerator.com/json/sentences.json\")\n",
    "type(response.text)\n",
    "data = json.loads(response.text)\n",
    "print(data[\"data\"])\n",
    "sentences = data[\"data\"]\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('data/invalid-sentences.csv', 'x+', newline='') as file:\n",
    "  writer = csv.writer(file)\n",
    "  \n",
    "  for idx in range(len(sentences)):\n",
    "    writer.writerow([sentences[idx][\"sentence\"], 0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "response = requests.get(\"https://www.randomsentencegen.com/sentence-with-startup\")\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "ideas = soup.findAll('tr', class_='item')\n",
    "idea_descriptions = [desc.text for desc in ideas]\n",
    "print(idea_descriptions, len(idea_descriptions))\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('data/invalid-sentences.csv', 'x+', newline='') as file:\n",
    "  writer = csv.writer(file)\n",
    "  \n",
    "  for idx in range(len(sentences)):\n",
    "    writer.writerow([sentences[idx][\"sentence\"], 0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating En to Vie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "inv = pd.read_csv(\"data/invalid_sentences.csv\", delimiter=';')\n",
    "# invalid_1 = pd.read_csv(\"data/random_str.csv\", delimiter=',')\n",
    "semantic = pd.read_csv(\"data/valid_ideas.csv\", delimiter=';', usecols=['sentence', 'spam'])\n",
    "\n",
    "inv.head()\n",
    "semantic.head()\n",
    "\n",
    "frames = [inv, semantic]\n",
    " \n",
    "df = pd.concat(frames)\n",
    "en_sent = inv.sentence.to_list()\n",
    "en_sent_1 = semantic.sentence.to_list()\n",
    "\n",
    "en_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the file path\n",
    "# file_path = \"data/vie/text_valid.csv\"\n",
    "\n",
    "def write_to_file(sentence, label, file_path):\n",
    "# Check if the file exists\n",
    "  if os.path.exists(file_path):\n",
    "      # File exists, open in append mode\n",
    "      with open(file_path, \"a\") as file:\n",
    "          file.write(f'{sentence};{label}\\n')\n",
    "  else:\n",
    "      # File doesn't exist, create and write content\n",
    "      with open(file_path, \"w\") as file:\n",
    "          file.write(f'{sentence};{label}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(\"hello\", 1, 'data/vie/testttt.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Vie SPAM Idea Dataset to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "model_name = \"VietAI/envit5-translation\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"data/vie/spam.csv\"\n",
    "\n",
    "# Write Vie Spam Dataset to CSV\n",
    "for sent in en_sent:\n",
    "  sentence = \"en: \" + sent\n",
    "  inputs = [sentence]\n",
    "  outputs = model.generate(tokenizer(inputs, return_tensors=\"pt\", padding=True).input_ids, max_length=512)\n",
    "  vie_trans = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0][4:]\n",
    "  write_to_file(vie_trans,0, file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Vie Valid Idea Description Dataset to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"VietAI/envit5-translation\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"data/vie/valid_idea.csv\"\n",
    "\n",
    "# Write Vie Valid Idea Description Dataset to CSV\n",
    "for sent in en_sent_1:\n",
    "  sentence = \"en: \" + sent\n",
    "  inputs = [sentence]\n",
    "  outputs = model.generate(tokenizer(inputs, return_tensors=\"pt\", padding=True).input_ids, max_length=512)\n",
    "  vie_trans = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0][4:]\n",
    "  write_to_file(vie_trans,1,file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhoBERT Embedding - Vietnamese Spam Filtering\n",
    "\n",
    "[Hugging Face - PhoBERT](https://huggingface.co/docs/transformers/model_doc/phobert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "# INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!\n",
    "line = \"Tôi là sinh_viên trường đại_học Công_nghệ .\"\n",
    "\n",
    "input_ids = torch.tensor([tokenizer.encode(line)])\n",
    "\n",
    "with torch.no_grad():\n",
    "  features = phobert(input_ids) # Models outputs are now tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_sentence = 'Dịch vụ đặt vé trực tuyến của chúng tôi, EasyBooking, giúp bạn dễ dàng tìm kiếm, so sánh và đặt vé máy bay, khách sạn và tour du lịch. Với giao diện thân thiện và tính năng tiện lợi, chúng tôi mang đến trải nghiệm đặt vé nhanh chóng và tiết kiệm thời gian cho bạn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import pos_tag, chunk, classify\n",
    "\n",
    "pos_str = pos_tag(tag_sentence)\n",
    "text_group = set([word for word,tag,num in chunk(tag_sentence) if tag == 'Np' or tag == 'N' or tag == 'V'])\n",
    "# class_of_text = classify(tag_sentence)\n",
    "nouns = set([word for word,tag in pos_str if tag == 'Np' or tag == 'N'])\n",
    "print(text_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gibberish-detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download https://metatext.io/datasets/cc100-vietnamese and merge all txt file text >> WIKI.txt\n",
    "\n",
    "!gibberish-detector train data/WIKI.txt > vie.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gibberish_detector import detector\n",
    "\n",
    "# open('data/WIKI.txt')\n",
    "\n",
    "Detector = detector.create_from_model('vie.model')\n",
    "print(Detector.is_gibberish('Chào, tớ là osijd lldkn qwoie')) #Gibberish"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
