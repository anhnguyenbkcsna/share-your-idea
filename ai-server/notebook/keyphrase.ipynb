{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Extractor\n",
    "\n",
    "[KE](https://github.com/protonx-tf-05-projects/vn-extract-keywords/blob/main/generate_keywords.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install keyphrase-vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install py_vncorenlp sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_vncorenlp\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "stop_words = []\n",
    "with open('../data/vietnamese-stopwords.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        stop_words.append(line.strip())\n",
    "\n",
    "doc = 'Túi xách là một trong những vật dụng cần thiết của hầu hết chị em phụ nữ mỗi khi ra đường.'\n",
    "\n",
    "def removeStopWords(o_sen):\n",
    "    words = [word for word in o_sen.split() if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "py_vncorenlp.download_model(save_dir=os.path.abspath('./vncorenlp'))\n",
    "\n",
    "# Load the word and sentence segmentation component\n",
    "rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir=os.path.abspath('./vncorenlp'))\n",
    "\n",
    "doc_segmented = rdrsegmenter.word_segment(doc)\n",
    "# Extract candidate words/phrases\n",
    "\n",
    "count = CountVectorizer(ngram_range=(1,1)).fit([removeStopWords(doc_segmented[0])])\n",
    "candidates = count.get_feature_names()\n",
    "\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n",
    "\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "\n",
    "top_n = 10\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import underthesea\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "stop_words = []\n",
    "with open('../data/vietnamese-stopwords.txt', encoding='utf8') as f:\n",
    "  for line in f:\n",
    "    stop_words.append(line.strip().replace(' ', '_'))\n",
    "\n",
    "def remove_vie_stop_words(o_sen):\n",
    "  o_sen = o_sen.lower()\n",
    "  words = [word for word in o_sen.split() if word not in stop_words]\n",
    "  # print(words)\n",
    "  return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = '''Một nền tảng chung quy chính bản phần mềm dựa trên đám mây cho thị trường giáo dục chuyên nghiệp. \n",
    "Công ty khởi nghiệp này đang xây dựng cơ sở dữ liệu biểu đồ để phân tích dữ liệu, dữ liệu không gian địa lý \n",
    "và mô hình ngữ nghĩa. Họ cho biết nó sẽ cho phép các giảng viên phân tích sinh viên tốt hơn và cung cấp \n",
    "các công cụ giảng dạy tốt hơn.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer # No Vietnamese Spacy Pipeline\n",
    "\n",
    "doc_segmented_list = [underthesea.word_tokenize(sentence, format='text') for sentence in doc.split('.')]\n",
    "# doc_segmented = underthesea.word_tokenize(doc, format='text').split(' ')\n",
    "# Extract candidate words/phrases\n",
    "\n",
    "count = CountVectorizer(ngram_range=(1,1)).fit([remove_vie_stop_words(doc_segmented) for doc_segmented in doc_segmented_list])\n",
    "candidates = count.get_feature_names_out()\n",
    "# print(candidates)\n",
    "\n",
    "# model = SentenceTransformer('keepitreal/vietnamese-sbert')\n",
    "\n",
    "doc_embedding = model.encode(doc.split('.'))\n",
    "candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "top_n = 10\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "\n",
    "#ndarray\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "\n",
    "print(keywords)\n",
    "# ['cơ_sở_dữ_liệu', 'không_gian', 'giảng_dạy', 'dựa', 'nền_tảng', 'đám', 'phần_mềm', 'mô_hình_ngữ', 'giáo_dục', 'mây']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# https://huggingface.co/keepitreal/vietnamese-sbert\n",
    "# Load model from HuggingFace Hub\n",
    "# tokenizer = AutoTokenizer.from_pretrained('keepitreal/vietnamese-sbert')\n",
    "# model = AutoModel.from_pretrained('keepitreal/vietnamese-sbert')\n",
    "model = SentenceTransformer('keepitreal/vietnamese-sbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def phobert_sentence_embedding(raw_sent):\n",
    "  # Word Segmented raw_sent\n",
    "  text = word_tokenize(raw_sent, format=\"text\")\n",
    "  input_ids = torch.tensor([tokenizer.encode(line)])\n",
    "\n",
    "  #disables gradient calculation.\n",
    "  with torch.no_grad():\n",
    "    features = phobert(input_ids).last_hidden_state[:,0,:] # Models outputs are now tuples\n",
    "\n",
    "  # print(features.shape)\n",
    "  # torch.Size([1, 768])\n",
    "  return np.to_array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phobert_sentence_embedding(\"Chào em\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_from_doc(raw_doc, word2vec) -> list:\n",
    "  doc_segmented_list = [underthesea.word_tokenize(sentence, format='text') for sentence in raw_doc.split('.')]\n",
    "  count = CountVectorizer(ngram_range=(1,1))\n",
    "  matrix = count.fit_transform([remove_vie_stop_words(doc_segmented) for doc_segmented in doc_segmented_list])\n",
    "  candidates = count.get_feature_names_out()\n",
    "\n",
    "  words_embed = []\n",
    "  for candidate in candidates:\n",
    "    # try:\n",
    "    #   word_embedding = word2vec[candidate]\n",
    "    # except:\n",
    "      word_embedding = phobert_sentence_embedding(candidate)[0]\n",
    "      words_embed.append(word_embedding)\n",
    "\n",
    "\n",
    "  return candidates, torch.tensor(words_embed)\n",
    "  # return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/sonvx/word2vecVN?fbclid=IwAR3oRcIyVSajJrolyQ2wJXvj5p1AfxGETZgJtkv0QpJTiZY6QvX3Y6tkhrk\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import models\n",
    "\n",
    "word2vec_path='word2vec/baomoi.model.bin'\n",
    "word_vectors = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors[\"văn_bản\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(word_vectors[\"chào\"])\n",
    "# torch.Size([400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# https://medium.com/@eskandar.sahel/exploring-feature-extraction-techniques-for-natural-language-processing-46052ee6514\n",
    "def keyword_extractor(raw_doc, word2vec, top_n = 5):\n",
    "  candidates, candidates_embedding = get_features_from_doc(raw_doc, word2vec)\n",
    "  sentences = raw_doc.split('.')\n",
    "  keywords = set([])\n",
    "  for idx in range(len(sentences)):\n",
    "    raw_sentence = sentences[idx]\n",
    "    sentence_embedding = phobert_sentence_embedding(raw_sentence)\n",
    "\n",
    "    print(sentence_embedding.shape)\n",
    "    print('---\\n', candidates_embedding.shape)\n",
    "    cosine_similarity(sentence_embedding, candidates_embedding)\n",
    "    list_cand = [candidates[index] for index in distances.argsort()[idx][-top_n:]]\n",
    "    for cand in list_cand:\n",
    "      keywords.add(cand)\n",
    "\n",
    "  return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer # No Vietnamese Spacy Pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "doc_segmented_list = [underthesea.word_tokenize(sentence, format='text') for sentence in doc.split('.')]\n",
    "# doc_segmented = underthesea.word_tokenize(doc, format='text').split(' ')\n",
    "# Extract candidate words/phrases\n",
    "\n",
    "count = CountVectorizer(ngram_range=(1,1))\n",
    "matrix = count.fit_transform([remove_vie_stop_words(doc_segmented) for doc_segmented in doc_segmented_list])\n",
    "candidates = count.get_feature_names_out()\n",
    "# print(\"Vocabulary: \", count.vocabulary_)\n",
    "print(\"Feature: \", candidates)\n",
    "\n",
    "# print(counts)\n",
    "\n",
    "# model = SentenceTransformer('keepitreal/vietnamese-sbert')\n",
    "\n",
    "doc_embedding = model.encode(doc.split('.'))\n",
    "# candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "# doc_embedding = [vie_sentence_embedding(sent) for sent in doc.split('.')]\n",
    "candidate_embeddings = model.encode(candidates)\n",
    "print(doc_embedding)\n",
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "# print(distances)\n",
    "# [ 0.26308233 -0.07585391 -0.18924099 ... -0.1127905  -0.17521721 0.08920351]\n",
    "#ndarray\n",
    "keywords = set([])\n",
    "\n",
    "for num in range(len(doc_embedding)):\n",
    "  list_cand = [candidates[index] for index in distances.argsort()[num][-top_n:]]\n",
    "  for x in list_cand:\n",
    "    keywords.add(x)\n",
    "\n",
    "print(keywords)\n",
    "# ['cơ_sở_dữ_liệu', 'không_gian', 'giảng_dạy', 'dựa', 'nền_tảng', 'đám', 'phần_mềm', 'mô_hình_ngữ', 'giáo_dục', 'mây']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_embeddings = model.encode(candidates)\n",
    "print(candidate_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_extractor(doc, word2vec=word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_embedding(sentence):\n",
    "  phobert_sentence_embedding(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
