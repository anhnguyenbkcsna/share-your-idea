{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas openai matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai\"[embeddings]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import openai\n",
    "\n",
    "from openai.embeddings_utils import (\n",
    "    get_embedding,\n",
    "    distances_from_embeddings,\n",
    "    tsne_components_from_embeddings,\n",
    "    chart_from_components,\n",
    "    indices_of_nearest_neighbors_from_distances,\n",
    ")\n",
    "\n",
    "# constants\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "openai.api_key=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short_description</th>\n",
       "      <th>category_list</th>\n",
       "      <th>category_groups_list</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Meridian Entertainment Group</th>\n",
       "      <td>Meridian Entertainment Group provides with eve...</td>\n",
       "      <td>Consulting,Events</td>\n",
       "      <td>Events,Media and Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RushRate</th>\n",
       "      <td>Ranking potential members of a fraternity.</td>\n",
       "      <td>Communities,Internet</td>\n",
       "      <td>Community and Lifestyle,Internet Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bishop Canevin High School</th>\n",
       "      <td>Bishop Canevin High School is a Catholic high ...</td>\n",
       "      <td>E-Learning,Education,Higher Education</td>\n",
       "      <td>Education,Software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Paladina Health</th>\n",
       "      <td>Paladina Health is an innovative employer-spon...</td>\n",
       "      <td>Health Care,Hospital,Medical,Personal Health</td>\n",
       "      <td>Health Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spire Group</th>\n",
       "      <td>Spire Group is a full-service accounting and c...</td>\n",
       "      <td>Accounting,Consulting</td>\n",
       "      <td>Financial Services,Professional Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Applied BioCode</th>\n",
       "      <td>Applied BioCode commercializes a multiplexing ...</td>\n",
       "      <td>Biotechnology,Genetics,Health Diagnostics,Life...</td>\n",
       "      <td>Biotechnology,Health Care,Science and Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DimDrop</th>\n",
       "      <td>DimDrop's technology enables teams to easily c...</td>\n",
       "      <td>Communities,Developer Tools,Enterprise Softwar...</td>\n",
       "      <td>Community and Lifestyle,Data and Analytics,Int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>specdox</th>\n",
       "      <td>Specfox is a web specification tool that allow...</td>\n",
       "      <td>Information Technology,Robotics,Software</td>\n",
       "      <td>Hardware,Information Technology,Science and En...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trainline Europe</th>\n",
       "      <td>Trainline (formerly Captain Train) sells train...</td>\n",
       "      <td>Internet,Ticketing,Travel</td>\n",
       "      <td>Events,Internet Services,Media and Entertainme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amplify ROI</th>\n",
       "      <td>Digital Marketing Services</td>\n",
       "      <td>Digital Marketing,SEO,Web Design</td>\n",
       "      <td>Design,Internet Services,Sales and Marketing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              short_description  \\\n",
       "name                                                                              \n",
       "Meridian Entertainment Group  Meridian Entertainment Group provides with eve...   \n",
       "RushRate                             Ranking potential members of a fraternity.   \n",
       "Bishop Canevin High School    Bishop Canevin High School is a Catholic high ...   \n",
       "Paladina Health               Paladina Health is an innovative employer-spon...   \n",
       "Spire Group                   Spire Group is a full-service accounting and c...   \n",
       "Applied BioCode               Applied BioCode commercializes a multiplexing ...   \n",
       "DimDrop                       DimDrop's technology enables teams to easily c...   \n",
       "specdox                       Specfox is a web specification tool that allow...   \n",
       "Trainline Europe              Trainline (formerly Captain Train) sells train...   \n",
       "Amplify ROI                                          Digital Marketing Services   \n",
       "\n",
       "                                                                  category_list  \\\n",
       "name                                                                              \n",
       "Meridian Entertainment Group                                  Consulting,Events   \n",
       "RushRate                                                   Communities,Internet   \n",
       "Bishop Canevin High School                E-Learning,Education,Higher Education   \n",
       "Paladina Health                    Health Care,Hospital,Medical,Personal Health   \n",
       "Spire Group                                               Accounting,Consulting   \n",
       "Applied BioCode               Biotechnology,Genetics,Health Diagnostics,Life...   \n",
       "DimDrop                       Communities,Developer Tools,Enterprise Softwar...   \n",
       "specdox                                Information Technology,Robotics,Software   \n",
       "Trainline Europe                                      Internet,Ticketing,Travel   \n",
       "Amplify ROI                                    Digital Marketing,SEO,Web Design   \n",
       "\n",
       "                                                           category_groups_list  \n",
       "name                                                                             \n",
       "Meridian Entertainment Group                     Events,Media and Entertainment  \n",
       "RushRate                              Community and Lifestyle,Internet Services  \n",
       "Bishop Canevin High School                                   Education,Software  \n",
       "Paladina Health                                                     Health Care  \n",
       "Spire Group                            Financial Services,Professional Services  \n",
       "Applied BioCode               Biotechnology,Health Care,Science and Engineering  \n",
       "DimDrop                       Community and Lifestyle,Data and Analytics,Int...  \n",
       "specdox                       Hardware,Information Technology,Science and En...  \n",
       "Trainline Europe              Events,Internet Services,Media and Entertainme...  \n",
       "Amplify ROI                        Design,Internet Services,Sales and Marketing  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data - the organizations.csv doesn't have \n",
    "import pandas as pd\n",
    "\n",
    "dataset_path = \"data/org_short.csv\"\n",
    "usecols = [\"name\", \"short_description\", \"category_list\", \"category_groups_list\"]\n",
    "df = pd.read_csv(dataset_path, index_col=\"name\", usecols=usecols)\n",
    "\n",
    "# print dataframe\n",
    "n_examples = 10\n",
    "df.head(n_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish a cache of embeddings to avoid recomputing\n",
    "# cache is a dict of tuples (text, model) -> embedding, saved as a pickle file\n",
    "\n",
    "# set path to embedding cache\n",
    "embedding_cache_path = \"data/recommendations_embeddings_cache.pkl\"\n",
    "\n",
    "# load the cache if it exists, and save a copy to disk\n",
    "try:\n",
    "    embedding_cache = pd.read_pickle(embedding_cache_path)\n",
    "except FileNotFoundError:\n",
    "    embedding_cache = {}\n",
    "with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "    pickle.dump(embedding_cache, embedding_cache_file)\n",
    "\n",
    "# define a function to retrieve embeddings from the cache if present, and otherwise request via the API\n",
    "def embedding_from_string(\n",
    "    string: str,\n",
    "    model: str = EMBEDDING_MODEL,\n",
    "    embedding_cache=embedding_cache\n",
    ") -> list:\n",
    "    \"\"\"Return embedding of given string, using a cache to avoid recomputing.\"\"\"\n",
    "    if (string, model) not in embedding_cache.keys():\n",
    "        embedding_cache[(string, model)] = get_embedding(string, model)\n",
    "        with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "            pickle.dump(embedding_cache, embedding_cache_file)\n",
    "    return embedding_cache[(string, model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as an example, take the first description from the dataset\n",
    "example_string = df[\"short_description\"].values[0]\n",
    "print(f\"\\nExample string: {example_string}\")\n",
    "\n",
    "\n",
    "# You exceeded your current quota, please check your plan and billing details\n",
    "example_embedding = embedding_from_string(example_string)\n",
    "print(f\"\\nExample embedding: {example_embedding[:3]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_recommendations_from_strings(\n",
    "    strings: list[str],\n",
    "    index_of_source_string: int,\n",
    "    k_nearest_neighbors: int = 1,\n",
    "    model=EMBEDDING_MODEL,\n",
    ") -> list[int]:\n",
    "    \"\"\"Print out the k nearest neighbors of a given string.\"\"\"\n",
    "    # get embeddings for all strings\n",
    "    embeddings = [embedding_from_string(string, model=model) for string in strings]\n",
    "    # get the embedding of the source string\n",
    "    query_embedding = embeddings[index_of_source_string]\n",
    "    # get distances between the source embedding and other embeddings (function from embeddings_utils.py)\n",
    "    distances = distances_from_embeddings(query_embedding, embeddings, distance_metric=\"cosine\")\n",
    "    # get indices of nearest neighbors (function from embeddings_utils.py)\n",
    "    indices_of_nearest_neighbors = indices_of_nearest_neighbors_from_distances(distances)\n",
    "\n",
    "    # print out source string\n",
    "    query_string = strings[index_of_source_string]\n",
    "    print(f\"Source string: {query_string}\")\n",
    "    # print out its k nearest neighbors\n",
    "    k_counter = 0\n",
    "    for i in indices_of_nearest_neighbors:\n",
    "        # skip any strings that are identical matches to the starting string\n",
    "        if query_string == strings[i]:\n",
    "            continue\n",
    "        # stop after printing out k articles\n",
    "        if k_counter >= k_nearest_neighbors:\n",
    "            break\n",
    "        k_counter += 1\n",
    "\n",
    "        # print out the similar strings and their distances\n",
    "        print(\n",
    "            f\"\"\"\n",
    "        --- Recommendation #{k_counter} (nearest neighbor {k_counter} of {k_nearest_neighbors}) ---\n",
    "        String: {strings[i]}\n",
    "        Distance: {distances[i]:0.3f}\"\"\"\n",
    "        )\n",
    "\n",
    "    return indices_of_nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_descriptions = df[\"short_description\"].tolist()\n",
    "\n",
    "tony_blair_articles = print_recommendations_from_strings(\n",
    "    strings=article_descriptions,  # let's base similarity off of the article description\n",
    "    index_of_source_string=0,  # let's look at articles similar to the first one about Tony Blair\n",
    "    k_nearest_neighbors=5,  # let's look at the 5 most similar articles\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEYWORDS - ENTITY EXTRACTOR\n",
    "### RAKE vs KeyBERT\n",
    "[https://viblo.asia/p/keyword-extraction-giai-phap-nhanh-cho-chon-loc-thong-tin-gDVK2P4vlLj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rake_nltk keybert flair\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/keybert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "# Create your LLM\n",
    "\n",
    "doc = \"\"\"\n",
    "         Supervised learning is the machine learning task of learning a function that\n",
    "         maps an input to an output based on example input-output pairs. It infers a\n",
    "         function from labeled training data consisting of a set of training examples.\n",
    "         In supervised learning, each example is a pair consisting of an input object\n",
    "         (typically a vector) and a desired output value (also called the supervisory signal).\n",
    "         A supervised learning algorithm analyzes the training data and produces an inferred function,\n",
    "         which can be used for mapping new examples. An optimal scenario will allow for the\n",
    "         algorithm to correctly determine the class labels for unseen instances. This requires\n",
    "         the learning algorithm to generalize from the training data to unseen situations in a\n",
    "         'reasonable' way (see inductive bias).\n",
    "      \"\"\"\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(doc)\n",
    "keywords = kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english',\n",
    "                              use_maxsum=True, nr_candidates=20, top_n=5)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from keybert import KeyBERT\n",
    "\n",
    "MY_DOCUMENTS =\"\"\"\n",
    "An innovative idea for a health business could be the development of a personalized health \n",
    "and wellness platform that utilizes artificial intelligence and data analytics to provide tailored \n",
    "recommendations and interventions for individuals. Here's how it could work:\n",
    "\n",
    "Comprehensive Health Assessment: The platform would start by collecting extensive information \n",
    "\n",
    "about the individual's health, including medical history, lifestyle habits, genetic data \n",
    "(if available), and real-time data from wearable devices and health trackers.\n",
    "\n",
    "AI-Powered Analysis: Using advanced machine learning algorithms, the platform would analyze \n",
    "the collected data to identify patterns, correlations, and risk factors associated with the\n",
    " individual's health status. This analysis would provide valuable insights into their overall well-being\n",
    "  and potential areas for improvement.\n",
    "\n",
    "Personalized Recommendations: Based on the analysis, the platform would generate \n",
    "personalized recommendations for the individual, including suggestions for exercise routines,\n",
    " nutrition plans, stress management techniques, and preventive screenings. These recommendations would be \n",
    " tailored to the individual's specific needs, goals, and preferences.\n",
    "\"\"\"\n",
    "# Extract embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "kw_model = KeyBERT(model=model)\n",
    "mini_keywords = kw_model.extract_keywords(MY_DOCUMENTS, keyphrase_ngram_range=(1, 3), stop_words='english',\n",
    "                              use_maxsum=True, nr_candidates=20, top_n=15, highlight=True)\n",
    "# print(mini_keywords)\n",
    "\n",
    "# Roberta\n",
    "roberta = TransformerDocumentEmbeddings('roberta-base')\n",
    "kw_model = KeyBERT(model=model)\n",
    "keywords = kw_model.extract_keywords(MY_DOCUMENTS, keyphrase_ngram_range=(1, 5), stop_words='english',\n",
    "                              use_maxsum=True, nr_candidates=20, top_n=15)\n",
    "print(keywords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. RAKE - Text Calculation Keyword Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "r = Rake()\n",
    "my_text = \"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him  seriously.\"\n",
    "r.extract_keywords_from_text(MY_DOCUMENTS)\n",
    "keywordList           = []\n",
    "rankedList            = r.get_ranked_phrases_with_scores()\n",
    "for keyword in rankedList:\n",
    "  keyword_updated       = keyword[1].split()\n",
    "  keyword_updated_string    = \" \".join(keyword_updated[:2])\n",
    "  keywordList.append(keyword_updated_string)\n",
    "print(keywordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>niche</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Technology and Information Technology</td>\n",
       "      <td>Software Development and IT Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Technology and Information Technology</td>\n",
       "      <td>Electronics and Hardware Manufacturing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Technology and Information Technology</td>\n",
       "      <td>Computer Hardware and Software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Technology and Information Technology</td>\n",
       "      <td>Internet Services and E-commerce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Technology and Information Technology</td>\n",
       "      <td>Artificial Intelligence and Machine Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Technology and Information Technology</td>\n",
       "      <td>Cybersecurity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Technology and Information Technology</td>\n",
       "      <td>Telecommunications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Healthcare and Pharmaceutical</td>\n",
       "      <td>Pharmaceuticals and Biotechnology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Healthcare and Pharmaceutical</td>\n",
       "      <td>Healthcare Services and Hospitals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Healthcare and Pharmaceutical</td>\n",
       "      <td>Medical Devices and Equipment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Healthcare and Pharmaceutical</td>\n",
       "      <td>Health and Wellness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Energy and Utilities</td>\n",
       "      <td>Renewable Energy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Energy and Utilities</td>\n",
       "      <td>Oil and Gas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Energy and Utilities</td>\n",
       "      <td>Power Generation and Distribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Energy and Utilities</td>\n",
       "      <td>Utilities and Energy Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Manufacturing and Industrial</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Manufacturing and Industrial</td>\n",
       "      <td>Automotive and Transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Manufacturing and Industrial</td>\n",
       "      <td>Aerospace and Defense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Manufacturing and Industrial</td>\n",
       "      <td>Chemicals and Materials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Manufacturing and Industrial</td>\n",
       "      <td>Heavy Machinery and Equipment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 category  \\\n",
       "0   Technology and Information Technology   \n",
       "1   Technology and Information Technology   \n",
       "2   Technology and Information Technology   \n",
       "3   Technology and Information Technology   \n",
       "4   Technology and Information Technology   \n",
       "5   Technology and Information Technology   \n",
       "6   Technology and Information Technology   \n",
       "7           Healthcare and Pharmaceutical   \n",
       "8           Healthcare and Pharmaceutical   \n",
       "9           Healthcare and Pharmaceutical   \n",
       "10          Healthcare and Pharmaceutical   \n",
       "11                   Energy and Utilities   \n",
       "12                   Energy and Utilities   \n",
       "13                   Energy and Utilities   \n",
       "14                   Energy and Utilities   \n",
       "15           Manufacturing and Industrial   \n",
       "16           Manufacturing and Industrial   \n",
       "17           Manufacturing and Industrial   \n",
       "18           Manufacturing and Industrial   \n",
       "19           Manufacturing and Industrial   \n",
       "\n",
       "                                            niche  \n",
       "0            Software Development and IT Services  \n",
       "1          Electronics and Hardware Manufacturing  \n",
       "2                  Computer Hardware and Software  \n",
       "3                Internet Services and E-commerce  \n",
       "4    Artificial Intelligence and Machine Learning  \n",
       "5                                   Cybersecurity  \n",
       "6                              Telecommunications  \n",
       "7               Pharmaceuticals and Biotechnology  \n",
       "8               Healthcare Services and Hospitals  \n",
       "9                   Medical Devices and Equipment  \n",
       "10                            Health and Wellness  \n",
       "11                               Renewable Energy  \n",
       "12                                    Oil and Gas  \n",
       "13              Power Generation and Distribution  \n",
       "14                  Utilities and Energy Services  \n",
       "15                                                 \n",
       "16                  Automotive and Transportation  \n",
       "17                          Aerospace and Defense  \n",
       "18                        Chemicals and Materials  \n",
       "19                  Heavy Machinery and Equipment  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_industry_path = \"data/industry_categories.csv\"\n",
    "industry = pd.read_csv(dataset_industry_path)\n",
    "industry.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json not found in HuggingFace Hub.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Called a TensorFlow-specific function but could not import it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m \u001b[39mimport\u001b[39;00m from_pretrained_keras\n\u001b[0;32m----> 3\u001b[0m model \u001b[39m=\u001b[39m from_pretrained_keras(\u001b[39m\"\u001b[39;49m\u001b[39mkeras-io/bert-semantic-similarity\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Code/Graduation/share-your-idea/venv/lib/python3.11/site-packages/huggingface_hub/keras_mixin.py:273\u001b[0m, in \u001b[0;36mfrom_pretrained_keras\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained_keras\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mKerasModelHubMixin\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    218\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39m    Instantiate a pretrained Keras model from a pre-trained model from the Hub.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39m    The model is expected to be in `SavedModel` format.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39m    </Tip>\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m     \u001b[39mreturn\u001b[39;00m KerasModelHubMixin\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Code/Graduation/share-your-idea/venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Code/Graduation/share-your-idea/venv/lib/python3.11/site-packages/huggingface_hub/hub_mixin.py:159\u001b[0m, in \u001b[0;36mModelHubMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, force_download, resume_download, proxies, token, cache_dir, local_files_only, revision, **model_kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m         config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m    157\u001b[0m     model_kwargs\u001b[39m.\u001b[39mupdate({\u001b[39m\"\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m\"\u001b[39m: config})\n\u001b[0;32m--> 159\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m    160\u001b[0m     model_id\u001b[39m=\u001b[39;49m\u001b[39mstr\u001b[39;49m(model_id),\n\u001b[1;32m    161\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    162\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    163\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    164\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    165\u001b[0m     resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    166\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    167\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    168\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    169\u001b[0m )\n",
      "File \u001b[0;32m~/Code/Graduation/share-your-idea/venv/lib/python3.11/site-packages/huggingface_hub/keras_mixin.py:459\u001b[0m, in \u001b[0;36mKerasModelHubMixin._from_pretrained\u001b[0;34m(cls, model_id, revision, cache_dir, force_download, proxies, resume_download, local_files_only, token, **model_kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCalled a TensorFlow-specific function but could not import it.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    461\u001b[0m \u001b[39m# TODO - Figure out what to do about these config values. Config is not going to be needed to load model\u001b[39;00m\n\u001b[1;32m    462\u001b[0m cfg \u001b[39m=\u001b[39m model_kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mImportError\u001b[0m: Called a TensorFlow-specific function but could not import it."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import from_pretrained_keras\n",
    "\n",
    "model = from_pretrained_keras(\"keras-io/bert-semantic-similarity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/22/50/1e211cbb5e1f52e55eeae1605789c9d24403962d37581cf0deb3e6b33377/tensorflow-2.14.0-cp311-cp311-macosx_10_15_x86_64.whl.metadata\n",
      "  Downloading tensorflow-2.14.0-cp311-cp311-macosx_10_15_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=23.5.26 from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m246.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting h5py>=2.9.0 (from tensorflow)\n",
      "  Obtaining dependency information for h5py>=2.9.0 from https://files.pythonhosted.org/packages/06/3e/935ebbd6cccd0b7965aaaceebabce979e032f15eacfeca12276e790a3e8b/h5py-3.9.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading h5py-3.9.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/c9/ea/fe2a69cc6cfebf7c7ee8a6357566fc1cbb91632bde5869b669a396accb5f/libclang-16.0.6-py2.py3-none-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading libclang-16.0.6-py2.py3-none-macosx_10_9_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes==0.2.0 (from tensorflow)\n",
      "  Obtaining dependency information for ml-dtypes==0.2.0 from https://files.pythonhosted.org/packages/15/da/43bee505963da0c730ee50e951c604bfdb90d4cccc9c0044c946b10e68a7/ml_dtypes-0.2.0-cp311-cp311-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading ml_dtypes-0.2.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.23.5 in ./venv/lib/python3.11/site-packages (from tensorflow) (1.26.0)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in ./venv/lib/python3.11/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./venv/lib/python3.11/site-packages (from tensorflow) (4.24.4)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.11/site-packages (from tensorflow) (68.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./venv/lib/python3.11/site-packages (from tensorflow) (4.8.0)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow)\n",
      "  Obtaining dependency information for wrapt<1.15,>=1.11.0 from https://files.pythonhosted.org/packages/e7/f9/8c078b4973604cd968b23eb3dff52028b5c48f2a02c4f1f975f4d5e344d1/wrapt-1.14.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading wrapt-1.14.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/3d/34/252794e3f737594f8ac4ac9f2ee9ba7b806b6825832af3ff9b2fd893ce8f/tensorflow_io_gcs_filesystem-0.34.0-cp311-cp311-macosx_10_14_x86_64.whl.metadata\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp311-cp311-macosx_10_14_x86_64.whl.metadata (14 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/bb/1c/4741e490b93488e077e36543b0acbfece41314b5a1ffae05bc7e2e9b3375/grpcio-1.59.0-cp311-cp311-macosx_10_10_universal2.whl.metadata\n",
      "  Downloading grpcio-1.59.0-cp311-cp311-macosx_10_10_universal2.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.15,>=2.14 (from tensorflow)\n",
      "  Obtaining dependency information for tensorboard<2.15,>=2.14 from https://files.pythonhosted.org/packages/73/a2/66ed644f6ed1562e0285fcd959af17670ea313c8f331c46f79ee77187eb9/tensorboard-2.14.1-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/d1/da/4f264c196325bb6e37a6285caec5b12a03def489b57cc1fdac02bb6272cd/tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.15,>=2.14.0 (from tensorflow)\n",
      "  Obtaining dependency information for keras<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/fe/58/34d4d8f1aa11120c2d36d7ad27d0526164b1a8ae45990a2fede31d0e59bf/keras-2.14.0-py3-none-any.whl.metadata\n",
      "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Obtaining dependency information for wheel<1.0,>=0.23.0 from https://files.pythonhosted.org/packages/b8/8b/31273bf66016be6ad22bb7345c37ff350276cfd46e389a0c2ac5da9d9073/wheel-0.41.2-py3-none-any.whl.metadata\n",
      "  Using cached wheel-0.41.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.15,>=2.14->tensorflow)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/d7/88/1826b0c047c48763b36ed854a984127b430a16b70003155d7b19975f1d59/google_auth-2.23.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.23.2-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.15,>=2.14->tensorflow)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/bb/c1/50caaec6cadc1c6adc8fe351e03bd646d6e4dd17f55fca0f4c8d7ea8d3e9/Markdown-3.5-py3-none-any.whl.metadata\n",
      "  Downloading Markdown-3.5-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.15,>=2.14->tensorflow)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/37/52/da3f777c56fdcf3a484493ce58403c1af709709cd05dab9894980fc8d962/tensorboard_data_server-0.7.1-py3-none-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-macosx_10_9_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.15,>=2.14->tensorflow)\n",
      "  Obtaining dependency information for werkzeug>=1.0.1 from https://files.pythonhosted.org/packages/b6/a5/54b01f663d60d5334f6c9c87c26274e94617a4fd463d812463626423b10d/werkzeug-3.0.0-py3-none-any.whl.metadata\n",
      "  Downloading werkzeug-3.0.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (1.26.17)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venv/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow)\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.14.0-cp311-cp311-macosx_10_15_x86_64.whl (229.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.7/229.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.2.0-cp311-cp311-macosx_10_9_universal2.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Downloading grpcio-1.59.0-cp311-cp311-macosx_10_10_universal2.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.9.0-cp311-cp311-macosx_10_9_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-16.0.6-py2.py3-none-macosx_10_9_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.34.0-cp311-cp311-macosx_10_14_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-macosx_10_9_x86_64.whl (35 kB)\n",
      "Downloading google_auth-2.23.2-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.0/182.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.5-py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.1-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.0-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.6/226.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached wheel-0.41.2-py3-none-any.whl (64 kB)\n",
      "Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, oauthlib, ml-dtypes, markdown, keras, h5py, grpcio, google-pasta, gast, cachetools, absl-py, rsa, requests-oauthlib, pyasn1-modules, astunparse, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.15.0\n",
      "    Uninstalling wrapt-1.15.0:\n",
      "      Successfully uninstalled wrapt-1.15.0\n",
      "Successfully installed absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.23.2 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.59.0 h5py-3.9.0 keras-2.14.0 libclang-16.0.6 markdown-3.5 ml-dtypes-0.2.0 oauthlib-3.2.2 opt-einsum-3.3.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.14.1 tensorboard-data-server-0.7.1 tensorflow-2.14.0 tensorflow-estimator-2.14.0 tensorflow-io-gcs-filesystem-0.34.0 termcolor-2.3.0 werkzeug-3.0.0 wheel-0.41.2 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
