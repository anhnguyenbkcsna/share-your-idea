{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Vie SPAM Idea Dataset to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "model_name = \"VietAI/envit5-translation\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"data/vie/spam.csv\"\n",
    "\n",
    "# Write Vie Spam Dataset to CSV\n",
    "for sent in en_sent:\n",
    "  sentence = \"en: \" + sent\n",
    "  inputs = [sentence]\n",
    "  outputs = model.generate(tokenizer(inputs, return_tensors=\"pt\", padding=True).input_ids, max_length=512)\n",
    "  vie_trans = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0][4:]\n",
    "  write_to_file(vie_trans,0, file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Vie Valid Idea Description Dataset to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"VietAI/envit5-translation\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"data/vie/valid_idea.csv\"\n",
    "\n",
    "# Write Vie Valid Idea Description Dataset to CSV\n",
    "for sent in en_sent_1:\n",
    "  sentence = \"en: \" + sent\n",
    "  inputs = [sentence]\n",
    "  outputs = model.generate(tokenizer(inputs, return_tensors=\"pt\", padding=True).input_ids, max_length=512)\n",
    "  vie_trans = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0][4:]\n",
    "  write_to_file(vie_trans,1,file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering by LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "invalid_0 = pd.read_csv(\"data/invalid_sentences.csv\", delimiter=';')\n",
    "invalid_1 = pd.read_csv(\"data/random_str.csv\", delimiter=',')\n",
    "semantic = pd.read_csv(\"data/valid_ideas.csv\", delimiter=';', usecols=['sentence', 'spam'])\n",
    "\n",
    "invalid_0.head()\n",
    "invalid_1.head()\n",
    "semantic.head()\n",
    "\n",
    "frames = [invalid_0, semantic, invalid_1]\n",
    " \n",
    "df = pd.concat(frames)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Tensorflow on M1: https://medium.com/@sorenlind/tensorflow-with-gpu-support-on-apple-silicon-mac-with-homebrew-and-without-conda-miniforge-915b2f15425b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dependency of the preprocessing for BERT inputs\n",
    "!pip3 install -U tensorflow-macos tensorflow-metal\n",
    "!pip3 install plotlib tf-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 show tensorflow\n",
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.test.is_gpu_available()\n",
    "# from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Create test and train samples from one dataframe with pandas?\n",
    "train_data, test_data = train_test_split(df, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = train_data.values.tolist()\n",
    "# print(train_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMBEDDING LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pandas numpy seaborn matplotlib scikit-learn keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "# tf.keras.optimizers.legacy.RMSprop\n",
    "from keras.src.optimizers.legacy.rmsprop import RMSprop\n",
    "from keras.src.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "invalid_0 = pd.read_csv(\"data/invalid_sentences.csv\", delimiter=';')\n",
    "invalid_1 = pd.read_csv(\"data/random_str.csv\", delimiter=',')\n",
    "semantic = pd.read_csv(\"data/valid_ideas.csv\", delimiter=';', usecols=['sentence', 'spam'])\n",
    "\n",
    "invalid_0.head()\n",
    "invalid_1.head()\n",
    "semantic.head()\n",
    "\n",
    "frames = [invalid_0, semantic, invalid_1]\n",
    " \n",
    "df = pd.concat(frames)\n",
    "\n",
    "df = df.reset_index()\n",
    "# df[100:120]\n",
    "# df.index.duplicated()\n",
    "df.index\n",
    "# df.loc[df.index.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df, x=\"spam\")\n",
    "plt.xlabel('Label')\n",
    "plt.title('Number of ham (1) and spam (0) ideas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.sentence\n",
    "Y = df.spam\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "Y = Y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "max_len = 150\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "  inputs = Input(name='inputs',shape=[max_len])\n",
    "  layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "  layer = LSTM(64)(layer)\n",
    "  layer = Dense(256,name='FC1')(layer)\n",
    "  layer = Activation('relu')(layer)\n",
    "  layer = Dropout(0.5)(layer)\n",
    "  layer = Dense(1,name='out_layer')(layer)\n",
    "  layer = Activation('sigmoid')(layer)\n",
    "  model = Model(inputs=inputs,outputs=layer)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(sequences_matrix,Y_train,batch_size=128,epochs=5,\n",
    "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(test_sequences_matrix,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx = 10\n",
    "prediction = model.predict(test_sequences_matrix[word_idx:word_idx+10])\n",
    "print(prediction, X_test[word_idx:word_idx+10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input(new_sentence, ai_model):\n",
    "  new_sentence = new_sentence + \".\"\n",
    "  test_sequences = tok.texts_to_sequences([new_sentence])\n",
    "  test_sequences_matrix = sequence.pad_sequences(test_sequences, maxlen=max_len)\n",
    "  prediction = ai_model.predict(test_sequences_matrix)\n",
    "  label = \"spam\"\n",
    "  if prediction > 0.5:\n",
    "    label = \"innovative idea\"\n",
    "  print(prediction, new_sentence[:30] + '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input(\"Nghi is building an AI app for tracking habit automatically that use voice-over to record your daily activities\", model)\n",
    "input(\"Origin recipes for this manificient coffee shop\", model)\n",
    "input(\"This is the new way to play guitar\", model)\n",
    "input(\"This is the new book\", model)\n",
    "input(\"This is the new AI model helping homework completion\", model)\n",
    "input(\"We are operating new AI model to auto complete homework\", model)\n",
    "input(\"Instagram is so popular that now everyone is using, scrolling til tired\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input(\"SmartMirror Health Assistant: A cutting-edge innovation that combines a traditional mirror with advanced AI technology to create a personalized health monitoring system. The SmartMirror analyzes facial features and provides real-time health insights, such as heart rate, stress levels, and skin condition. It also offers personalized workout recommendations and nutrition tips based on individual goals. With voice control and integration with smart devices, the SmartMirror Health Assistant revolutionizes daily routines, empowers users to make informed health decisions, and promotes overall well-being in a seamless and interactive way.\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input(\"Paperclip Organizer: A simple and practical solution for keeping paperclips organized and easily accessible. This compact organizer features separate compartments for different sizes and colors of paperclips, preventing them from getting lost or tangled. Its sleek design takes up minimal desk space, offering convenience and efficiency for everyday office tasks.\", model)\n",
    "input(\"A budget-friendly hotel chain providing comfortable accommodations, basic amenities, and convenient locations near popular attractions.\", model)\n",
    "input(\"A travel agency offering pre-packaged vacation deals to popular tourist destinations, with standard itineraries and accommodations.\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = \"\"\"\n",
    "A travel agency that specializes in creating immersive cultural experiences for travelers\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input(docs, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhoBERT Embedding - Vietnamese Spam Filtering\n",
    "\n",
    "[Hugging Face - PhoBERT](https://huggingface.co/docs/transformers/model_doc/phobert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "# INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!\n",
    "line = \"Tôi là sinh_viên trường đại_học Công_nghệ .\"\n",
    "\n",
    "input_ids = torch.tensor([tokenizer.encode(line)])\n",
    "\n",
    "with torch.no_grad():\n",
    "  features = phobert(input_ids) # Models outputs are now tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_sentence = 'Dịch vụ đặt vé trực tuyến của chúng tôi, EasyBooking, giúp bạn dễ dàng tìm kiếm, so sánh và đặt vé máy bay, khách sạn và tour du lịch. Với giao diện thân thiện và tính năng tiện lợi, chúng tôi mang đến trải nghiệm đặt vé nhanh chóng và tiết kiệm thời gian cho bạn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import pos_tag, chunk, classify\n",
    "\n",
    "pos_str = pos_tag(tag_sentence)\n",
    "text_group = set([word for word,tag,num in chunk(tag_sentence) if tag == 'Np' or tag == 'N' or tag == 'V'])\n",
    "# class_of_text = classify(tag_sentence)\n",
    "nouns = set([word for word,tag in pos_str if tag == 'Np' or tag == 'N'])\n",
    "print(text_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viet IDEA Spam Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "abs_path = '/Users/macos/Code/Graduation/share-your-idea/ml-server/'\n",
    "invalid_1 = pd.read_csv(abs_path + \"data/random_str.csv\", delimiter=',')\n",
    "invalid_0 = pd.read_csv(abs_path + \"data/vie/spam.csv\", delimiter=';')\n",
    "semantic = pd.read_csv(abs_path + \"data/vie/valid_idea.csv\", delimiter=';', usecols=['sentence', 'spam'])\n",
    "\n",
    "invalid_0.head()\n",
    "invalid_1.head()\n",
    "semantic.head()\n",
    "\n",
    "frames = [invalid_0, semantic, invalid_1]\n",
    " \n",
    "df = pd.concat(frames)\n",
    "\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df, x=\"spam\")\n",
    "plt.xlabel('Label')\n",
    "plt.title('Number of ham (1) and spam (0) ideas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.sentence\n",
    "Y = df.spam\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "Y = Y.reshape(-1,1)\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "max_len = 150\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_train)\n",
    "sequences = tok.texts_to_sequences(X_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "  inputs = Input(name='inputs',shape=[max_len])\n",
    "  layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "  layer = LSTM(64)(layer)\n",
    "  layer = Dense(256,name='FC1')(layer)\n",
    "  layer = Activation('relu')(layer)\n",
    "  layer = Dropout(0.5)(layer)\n",
    "  layer = Dense(1,name='out_layer')(layer)\n",
    "  layer = Activation('sigmoid')(layer)\n",
    "  model = Model(inputs=inputs,outputs=layer)\n",
    "  model.load_weights(model_weights_file)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(learning_rate=1e-3),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(sequences_matrix,Y_train,batch_size=128,epochs=5,\n",
    "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(test_sequences_matrix,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx = 17\n",
    "prediction = model.predict(test_sequences_matrix[word_idx:word_idx+5])\n",
    "print(prediction, X_test[word_idx:word_idx+5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "# save the iris classification model as a pickle file\n",
    "model_pkl_file = \"../src/model/spam_filtering.pkl\"  \n",
    "model_json_file = \"../src/model/spam_filtering.json\"\n",
    "model_weights_file = \"../src/model/weights.h5py\"\n",
    "\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(model_json_file, 'w') as file:  \n",
    "  file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(model_weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "save_model_file='../src/model/lstm'\n",
    "model.save(save_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input(new_sentence, ai_model):\n",
    "  new_sentence = new_sentence + \".\"\n",
    "  test_sequences = tok.texts_to_sequences([new_sentence])\n",
    "  test_sequences_matrix = sequence.pad_sequences(test_sequences, maxlen=max_len)\n",
    "  prediction = ai_model.predict(test_sequences_matrix)\n",
    "  label = \"SPAM\"\n",
    "  if prediction > 0.5:\n",
    "    label = \"VALID\"\n",
    "  # print(prediction, label, new_sentence[:30] + '...')\n",
    "  print(test_sequences_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = \"\"\"Một ống kính độ dài 2.5cm đang chỉa thẳng xuống mặt đất để đo đạc và chụp lại\"\"\" #Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = \"\"\"MR.Bean là một sản phẩm tái tạo năng lượng bằng hệ thống xử lí chất thải sinh học tự động\"\"\" #Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = \"\"\"Agelina - đứa con của công ty mẹ Egosh, là sàn thương mại điện tử nức tiếng nước Đức\"\"\" #Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = \"\"\"\"SmartMirror Health Monitor\" là một hệ thống kết hợp gương trang điểm thông minh với chức năng theo dõi sức khỏe. Gương sẽ hiển thị thông tin về nhịp tim, nhiệt độ cơ thể và các thông số sức khỏe khác. Ngoài ra, nó cũng cung cấp các gợi ý về dinh dưỡng và chế độ tập luyện dựa trên dữ liệu cá nhân. SmartMirror Health Monitor giúp người dùng theo dõi sức khỏe một cách thuận tiện trong khi trang điểm hoặc thay đồ hàng ngày.\"\"\" #Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = \"\"\"\"Ứng dụng kết hợp với cảm biến đất, ánh sáng và nước để giám sát và quản lý cây trồng. Người dùng sẽ nhận được thông báo để tưới nước, bón phân và chăm sóc cây đúng cách, tạo điều kiện tối ưu cho sự phát triển và sức khỏe của cây.\"\"\" #Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = \"\"\"Sản phẩm TechSmart đem đến cho bạn sự tiện ích và khả năng kết nối thông minh. Với tính năng đột phá và hiệu suất cao, chúng tôi giúp bạn trải nghiệm công nghệ tối ưu và tận hưởng cuộc sống hiện đại\"\"\" #Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input(docs, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibberish Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gibberish-detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gibberish-detector train data/WIKI.txt > vie.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gibberish_detector import detector\n",
    "\n",
    "# open('data/WIKI.txt')\n",
    "\n",
    "Detector = detector.create_from_model('vie.model')\n",
    "print(Detector.is_gibberish('Chào, tớ là osijd lldkn qwoie')) #Gibberish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from underthesea import chunk, pos_tag\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import models\n",
    "from gibberish_detector import detector\n",
    "import pickle\n",
    "\n",
    "model_json_file = \"../src/model/spam_filtering.json\"\n",
    "model_weights_file = \"../src/model/weights.h5py\"\n",
    "word2vec_path='../src/model/word2vec.vie.model.bin'\n",
    "gibberish_path='../src/model/gibberish.vie.model'\n",
    "max_words = 1000\n",
    "max_len = 150\n",
    "\n",
    "# with open(model_json_file, 'r') as file:  \n",
    "#   json_model = file.read()\n",
    "#   lstm_model = model_from_json(json_model)\n",
    "#   lstm_model.load_weights(model_weights_file)\n",
    "#   lstm_model.compile(loss='binary_crossentropy',optimizer=Adam(learning_rate=1e-3),metrics=['accuracy'])\n",
    "\n",
    "class SpamFiltering:\n",
    "  def __init__(self, lstm_model) -> None:\n",
    "    self.tokenizer = Tokenizer(num_words=max_words)\n",
    "    self.detector = detector.create_from_model(gibberish_path)\n",
    "    self.lstm_model = lstm_model\n",
    "\n",
    "  def is_idea_spam(self, idea_object):\n",
    "    desc = [idea_object.solution + '.']\n",
    "    print(desc)\n",
    "    if self.detector.is_gibberish(desc):\n",
    "      return \"SPAM\"\n",
    "    self.tokenizer.fit_on_texts(desc)\n",
    "    test_sequences = self.tokenizer.texts_to_sequences(desc)\n",
    "    test_sequences_matrix = sequence.pad_sequences(test_sequences, maxlen=max_len)\n",
    "    print(test_sequences)\n",
    "    prediction = self.lstm_model.predict(test_sequences_matrix)\n",
    "    # prediction = model.predict(test_sequences_matrix)\n",
    "    label = \"SPAM\"\n",
    "    if prediction > 0.5:\n",
    "      label = \"VALID\"\n",
    "    elif prediction > 0.3:\n",
    "      label = \"WARNING\"\n",
    "    print(prediction, f' -> {label} :: \"{desc[:30]}...\"')\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdeaObj:\n",
    "  def __init__(self,problem='',solution='') -> None:\n",
    "    self.problem=problem\n",
    "    self.solution=solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  try:\n",
    "    lstm_model=None\n",
    "    with open(model_json_file, 'r') as file:  \n",
    "      json_model = file.read()\n",
    "      lstm_model = model_from_json(json_model)\n",
    "      lstm_model.load_weights(model_weights_file).expect_partial()\n",
    "      # lstm_model.compile(loss='binary_crossentropy',optimizer=Adam(learning_rate=1e-3),metrics=['accuracy'])\n",
    "    spam = SpamFiltering(lstm_model)\n",
    "    problem='tài chính'\n",
    "    solution=\"Một ứng dụng cho phép người dùng quản lí tài chính cá nhân bằng trực quan hóa số liệu và hỗ trợ quyết định đầu tư, tiết kiệm.\"\n",
    "    solution=\"\"\"Ứng dụng kết hợp với cảm biến đất, ánh sáng và nước để giám sát và quản lý cây trồng. Người dùng sẽ nhận được thông báo để tưới nước, bón phân và chăm sóc cây đúng cách, tạo điều kiện tối ưu cho sự phát triển và sức khỏe của cây.\"\"\"\n",
    "    idea = IdeaObj(problem=problem, solution=solution)\n",
    "    spam.is_idea_spam(idea)\n",
    "  except ValueError as ve:\n",
    "    return str(ve)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a704de84d788518585a1734d7e07d73c8af2617ee4bc5ea96944d1c9b2e73160"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
