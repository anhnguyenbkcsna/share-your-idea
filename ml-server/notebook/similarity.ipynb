{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Similarity\n",
    "\n",
    "## Steps\n",
    "1. sentence -> PhoBERT Embedding -> sentence embedding\n",
    "2. sentence embedding cosine similarity\n",
    "\n",
    "3. word2vec\n",
    "\n",
    "## Fields\n",
    "1. problem - solution - domain - project_name\n",
    "2. customer behavior - problem\n",
    "3. outstands - solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,  218,    8,  649,  212,  956, 2413,    5,    2]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "# INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!\n",
    "line = \"Tôi là sinh_viên trường đại_học Công_nghệ .\"\n",
    "\n",
    "input_ids = torch.tensor([tokenizer.encode(line)])\n",
    "\n",
    "with torch.no_grad():\n",
    "  features = phobert(input_ids) # Models outputs are now tuples\n",
    "\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0, 13017,    88,     4,    68,     8,  5954,   649,   212,   850,\n",
      "          7939,     5,     2,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]])\n"
     ]
    }
   ],
   "source": [
    "from underthesea import word_tokenize\n",
    "\n",
    "max_len=100\n",
    "\n",
    "sentence=\"Chào bạn, mình là Nghi sinh viên trường Đại học Bách khoa.\"\n",
    "\n",
    "# Word Segmented Sentence\n",
    "sentence = word_tokenize(sentence, format=\"text\")\n",
    "\n",
    "# Tokenize: Padding -> Pytorch Tensor\n",
    "# https://huggingface.co/transformers/v3.3.1/internal/tokenization_utils.html\n",
    "sentence_tokenizer = tokenizer(text=sentence,padding='max_length', max_length=max_len) # tf, np, pt\n",
    "input_ids = torch.tensor([sentence_tokenizer.input_ids])\n",
    "attention_mask = sentence_tokenizer.attention_mask\n",
    "print(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phobert_padding_tokenizer(sentence, max_len=100):\n",
    "    # Word Segmented Sentence\n",
    "    text = word_tokenize(sentence, format=\"text\")\n",
    "    \n",
    "    encoding = tokenizer.encode_plus(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                add_special_tokens=True,\n",
    "                max_length=max_len,\n",
    "                padding='max_length',\n",
    "                return_attention_mask=True,\n",
    "                return_token_type_ids=False,\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "            \n",
    "    return {\n",
    "        'text': text,\n",
    "        'input_ids': encoding['input_ids'], #torch.Size([1, 100])\n",
    "        'attention_mask': encoding['attention_mask'], #torch.Size([1, 100])\n",
    "        # 'targets': torch.tensor(label, dtype=torch.long),\n",
    "    }       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phobert_sentence_embedding(sentence, max_len=100):\n",
    "    # Word Segmented Sentence\n",
    "    text = word_tokenize(sentence, format=\"text\")\n",
    "    input_ids = torch.tensor([tokenizer.encode(line)])\n",
    "\n",
    "    #disables gradient calculation.\n",
    "    with torch.no_grad():\n",
    "        features = phobert(input_ids).last_hidden_state[:,0,:] # Models outputs are now tuples\n",
    "\n",
    "    print(features.shape)\n",
    "    # torch.Size([1, 768])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# Output: the hidden state vector of pre-defined hidden size corresponding to each token in the input sequence.\n",
    "encoded_sequence = phobert_padding_tokenizer(sentence)\n",
    "input_ids = encoded_sequence['input_ids']\n",
    "attention_mask = encoded_sequence['attention_mask']\n",
    "\n",
    "doc = \"\"\"Đây là cô gái đến từ hôm qua\"\"\"\n",
    "phobert_sentence_embedding(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "[Github Word2Vec Vietnamese](https://github.com/sonvx/word2vecVN?fbclid=IwAR3JA6FwBTSotl6u_JkXBuHmaeTGRTmkWSo_zCjdqp0zArK2mUJ2tc15dvU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gensim pot #pot is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'word2vec/baomoi.model.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[1;32m      4\u001b[0m word2vec_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mword2vec/baomoi.model.bin\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m word_vectors \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39;49mKeyedVectors\u001b[39m.\u001b[39;49mload_word2vec_format(word2vec_path, binary\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Code/Graduation/share-your-idea/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1674\u001b[0m         \u001b[39mcls\u001b[39m, fname, fvocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m, unicode_errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, datatype\u001b[39m=\u001b[39mREAL, no_header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1676\u001b[0m     ):\n\u001b[1;32m   1677\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \n\u001b[1;32m   1679\u001b[0m \u001b[39m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[1;32m   1720\u001b[0m         \u001b[39mcls\u001b[39;49m, fname, fvocab\u001b[39m=\u001b[39;49mfvocab, binary\u001b[39m=\u001b[39;49mbinary, encoding\u001b[39m=\u001b[39;49mencoding, unicode_errors\u001b[39m=\u001b[39;49municode_errors,\n\u001b[1;32m   1721\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit, datatype\u001b[39m=\u001b[39;49mdatatype, no_header\u001b[39m=\u001b[39;49mno_header,\n\u001b[1;32m   1722\u001b[0m     )\n",
      "File \u001b[0;32m~/Code/Graduation/share-your-idea/.venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:2048\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2045\u001b[0m             counts[word] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(count)\n\u001b[1;32m   2047\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mloading projection weights from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, fname)\n\u001b[0;32m-> 2048\u001b[0m \u001b[39mwith\u001b[39;00m utils\u001b[39m.\u001b[39;49mopen(fname, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fin:\n\u001b[1;32m   2049\u001b[0m     \u001b[39mif\u001b[39;00m no_header:\n\u001b[1;32m   2050\u001b[0m         \u001b[39m# deduce both vocab_size & vector_size from 1st pass over file\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m         \u001b[39mif\u001b[39;00m binary:\n",
      "File \u001b[0;32m~/Code/Graduation/share-your-idea/.venv/lib/python3.10/site-packages/smart_open/smart_open_lib.py:177\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m transport_params \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     transport_params \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 177\u001b[0m fobj \u001b[39m=\u001b[39m _shortcut_open(\n\u001b[1;32m    178\u001b[0m     uri,\n\u001b[1;32m    179\u001b[0m     mode,\n\u001b[1;32m    180\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    181\u001b[0m     buffering\u001b[39m=\u001b[39;49mbuffering,\n\u001b[1;32m    182\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    183\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    184\u001b[0m     newline\u001b[39m=\u001b[39;49mnewline,\n\u001b[1;32m    185\u001b[0m )\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39mreturn\u001b[39;00m fobj\n",
      "File \u001b[0;32m~/Code/Graduation/share-your-idea/.venv/lib/python3.10/site-packages/smart_open/smart_open_lib.py:363\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m errors \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m    361\u001b[0m     open_kwargs[\u001b[39m'\u001b[39m\u001b[39merrors\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m errors\n\u001b[0;32m--> 363\u001b[0m \u001b[39mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[39m=\u001b[39;49mbuffering, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopen_kwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'word2vec/baomoi.model.bin'"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim import models\n",
    "\n",
    "word2vec_path='word2vec/baomoi.model.bin'\n",
    "word_vectors = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m word_vectors[\u001b[39m\"\u001b[39m\u001b[39mvăn_bản\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_vectors' is not defined"
     ]
    }
   ],
   "source": [
    "word_vectors[\"văn_bản\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "đặt\n",
      "<dịch_vụ> is 0.23475560545921326 similar to <khách_sạn>\n",
      "n_sim -> 0.2734\n"
     ]
    }
   ],
   "source": [
    "#https://radimrehurek.com/gensim/models/keyedvectors.html\n",
    "\n",
    "from underthesea import word_tokenize\n",
    "\n",
    "docs = word_tokenize(\"khách sạn du lịch máy bay đặt vé nghỉ dưỡng thư giãn\")\n",
    "print(word_vectors.doesnt_match(docs)) #đặt\n",
    "\n",
    "# sim = word_vectors.most_similar('yêu', topn=50)\n",
    "# print(sim)\n",
    "\n",
    "w1 = \"dịch_vụ\"\n",
    "w2 = \"khách_sạn\"\n",
    "result = word_vectors.similarity(w1, w2)\n",
    "print(f'<{w1}> is {result} similar to <{w2}>')\n",
    "\n",
    "# most_similar_key, similarity = result[0]  # look at the first match\n",
    "# print(f\"{most_similar_key}: {similarity:.4f}\")\n",
    "\n",
    "sentence_obama = word_tokenize('Obama là tổng thống da màu nỗi tiếng được phỏng vấn', format=\"text\").lower().split()\n",
    "sentence_president = 'Ngài Tổng thống chào báo chí ở Chicago'\n",
    "sentence_president = word_tokenize(sentence_president, format=\"text\").lower().split()\n",
    "\n",
    "#Compute the Word Mover’s Distance between two documents. \n",
    "# similarity = word_vectors.wmdistance(sentence_obama, sentence_president)\n",
    "# print(f\"{sentence_obama}\\n {sentence_president} \\n -> {similarity:.4f}\")\n",
    "\n",
    "\n",
    "similarity = word_vectors.n_similarity(['cháo', 'li'], ['nước', 'đồ_ăn'])\n",
    "print(f\"n_sim -> {similarity:.4f}\")\n",
    "\n",
    "\n",
    "# vector = word_vectors['chén']  # numpy vector of a word\n",
    "# vector.shape\n",
    "\n",
    "# vector = word_vectors.get_vector('văn', norm=True)\n",
    "# vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_extractor(sentence):\n",
    "  sent_embedding = phobert_sentence_embedding(sentence)\n",
    "  word_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_document_related(sent1, sent2, word2vec):\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'âm_thanh', 'được', 'bài', 'văn_bản', 'đồ', 'học', 'máy_tính', 'giảng', 'phương_tiện', 'họa', 'tập_trung', 'sự', 'người', 'đa', 'công_cụ', 'thu_hút', 'xây_dựng', 'hình_ảnh', 'giáo_viên'}\n",
      "{'Ứng_dụng', 'dạy_học', 'công_nghệ_thông_tin'}\n",
      "{'âm_thanh', 'được', 'bài', 'văn_bản', 'đồ', 'học', 'máy_tính', 'giảng', 'phương_tiện', 'họa', 'tập_trung', 'sự', 'người', 'đa', 'công_cụ', 'thu_hút', 'xây_dựng', 'hình_ảnh', 'giáo_viên'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5020064592761162"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1='Nhờ các công cụ đa phương tiện của máy tính như văn bản, đồ họa, hình ảnh, âm thanh, giáo viên sẽ xây dựng được bài giảng sinh động thu hút sự tập trung của người học'\n",
    "doc2='Ứng dụng công nghệ thông tin vào dạy học.'\n",
    "\n",
    "is_document_related(doc1, doc2, word_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gibberish_detector import detector\n",
    "\n",
    "# open('data/WIKI.txt')\n",
    "\n",
    "Detector = detector.create_from_model('vie-model/gibberish.vie.model')\n",
    "\n",
    "def is_gibberish(doc):\n",
    "  return Detector.is_gibberish(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamFiltering:\n",
    "  def __init__(self):\n",
    "    self.detector = detector.create_from_model('vie-model/gibberish.vie.model')\n",
    "\n",
    "  def is_idea_spam(self, idea_object):\n",
    "    solution = idea_object.solution\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a704de84d788518585a1734d7e07d73c8af2617ee4bc5ea96944d1c9b2e73160"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
