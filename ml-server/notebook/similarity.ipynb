{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Similarity\n",
    "\n",
    "## Steps\n",
    "1. sentence -> PhoBERT Embedding -> sentence embedding\n",
    "2. sentence embedding cosine similarity\n",
    "\n",
    "3. word2vec\n",
    "\n",
    "## Fields\n",
    "1. problem - solution - domain - project_name\n",
    "2. customer behavior - problem\n",
    "3. outstands - solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "# INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!\n",
    "line = \"Tôi là sinh_viên trường đại_học Công_nghệ .\"\n",
    "\n",
    "input_ids = torch.tensor([tokenizer.encode(line)])\n",
    "\n",
    "with torch.no_grad():\n",
    "  features = phobert(input_ids) # Models outputs are now tuples\n",
    "\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize\n",
    "\n",
    "max_len=100\n",
    "\n",
    "sentence=\"Chào bạn, mình là Nghi sinh viên trường Đại học Bách khoa.\"\n",
    "\n",
    "# Word Segmented Sentence\n",
    "sentence = word_tokenize(sentence, format=\"text\")\n",
    "\n",
    "# Tokenize: Padding -> Pytorch Tensor\n",
    "# https://huggingface.co/transformers/v3.3.1/internal/tokenization_utils.html\n",
    "sentence_tokenizer = tokenizer(text=sentence,padding='max_length', max_length=max_len) # tf, np, pt\n",
    "input_ids = torch.tensor([sentence_tokenizer.input_ids])\n",
    "attention_mask = sentence_tokenizer.attention_mask\n",
    "print(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phobert_tokenizer(sentence, max_len=100):\n",
    "    # Word Segmented Sentence\n",
    "    text = word_tokenize(sentence, format=\"text\")\n",
    "    \n",
    "    encoding = tokenizer.encode_plus(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                add_special_tokens=True,\n",
    "                max_length=max_len,\n",
    "                padding='max_length',\n",
    "                return_attention_mask=True,\n",
    "                return_token_type_ids=False,\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "            \n",
    "    return {\n",
    "        'text': text,\n",
    "        'input_ids': encoding['input_ids'], #torch.Size([1, 100])\n",
    "        'attention_mask': encoding['attention_mask'], #torch.Size([1, 100])\n",
    "        # 'targets': torch.tensor(label, dtype=torch.long),\n",
    "    }       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# Output: the hidden state vector of pre-defined hidden size corresponding to each token in the input sequence.\n",
    "encoded_sequence = phobert_tokenizer(sentence)\n",
    "input_ids = encoded_sequence['input_ids']\n",
    "attention_mask = encoded_sequence['attention_mask']\n",
    "\n",
    "phobert(input_ids, attention_mask, return_dict=False) # Dropout will errors if without return_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "[Github Word2Vec Vietnamese](https://github.com/sonvx/word2vecVN?fbclid=IwAR3JA6FwBTSotl6u_JkXBuHmaeTGRTmkWSo_zCjdqp0zArK2mUJ2tc15dvU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gensim pot #pot is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim import models\n",
    "\n",
    "word2vec_path='word2vec/baomoi.model.bin'\n",
    "word_vectors = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors[\"văn_bản\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "đặt\n",
      "<dịch_vụ> is 0.23475560545921326 similar to <khách_sạn>\n",
      "n_sim -> 0.2734\n"
     ]
    }
   ],
   "source": [
    "#https://radimrehurek.com/gensim/models/keyedvectors.html\n",
    "\n",
    "from underthesea import word_tokenize\n",
    "\n",
    "docs = word_tokenize(\"khách sạn du lịch máy bay đặt vé nghỉ dưỡng thư giãn\")\n",
    "print(word_vectors.doesnt_match(docs)) #đặt\n",
    "\n",
    "# sim = word_vectors.most_similar('yêu', topn=50)\n",
    "# print(sim)\n",
    "\n",
    "w1 = \"dịch_vụ\"\n",
    "w2 = \"khách_sạn\"\n",
    "result = word_vectors.similarity(w1, w2)\n",
    "print(f'<{w1}> is {result} similar to <{w2}>')\n",
    "\n",
    "# most_similar_key, similarity = result[0]  # look at the first match\n",
    "# print(f\"{most_similar_key}: {similarity:.4f}\")\n",
    "\n",
    "sentence_obama = word_tokenize('Obama là tổng thống da màu nỗi tiếng được phỏng vấn', format=\"text\").lower().split()\n",
    "sentence_president = 'Ngài Tổng thống chào báo chí ở Chicago'\n",
    "sentence_president = word_tokenize(sentence_president, format=\"text\").lower().split()\n",
    "\n",
    "#Compute the Word Mover’s Distance between two documents. \n",
    "# similarity = word_vectors.wmdistance(sentence_obama, sentence_president)\n",
    "# print(f\"{sentence_obama}\\n {sentence_president} \\n -> {similarity:.4f}\")\n",
    "\n",
    "\n",
    "similarity = word_vectors.n_similarity(['cháo', 'li'], ['nước', 'đồ_ăn'])\n",
    "print(f\"n_sim -> {similarity:.4f}\")\n",
    "\n",
    "\n",
    "# vector = word_vectors['chén']  # numpy vector of a word\n",
    "# vector.shape\n",
    "\n",
    "# vector = word_vectors.get_vector('văn', norm=True)\n",
    "# vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize\n",
    "from underthesea import pos_tag, chunk, classify\n",
    "\n",
    "def get_nouns_verbs(doc):\n",
    "  pos_str = pos_tag(sentence)\n",
    "  text_group = set([word.replace(' ', '_') for word,tag,num in chunk(sentence) if tag == 'Np' or tag == 'N' or tag == 'V'])\n",
    "  # print(text_group)\n",
    "  return text_group\n",
    "\n",
    "def is_document_related(doc1, doc2, word2vec):\n",
    "  if word_vectors.wmdistance(doc1, doc2) > 0.5:\n",
    "    return True\n",
    "  word_seg_list1 = get_nouns_verbs(doc1)\n",
    "  word_seg_list2 = get_nouns_verbs(doc2)\n",
    "\n",
    "  print(word_seg_list1)\n",
    "\n",
    "  res = []\n",
    "  for w1 in word_seg_list1:\n",
    "    for w2 in word_seg_list2:\n",
    "      try:\n",
    "        sim = word2vec.similarity(w1, w2)\n",
    "        if (sim >= 0.3):\n",
    "          res.append((w1, w2, sim))\n",
    "      except:\n",
    "        continue\n",
    "\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'âm_thanh', 'được', 'bài', 'văn_bản', 'đồ', 'học', 'máy_tính', 'giảng', 'phương_tiện', 'họa', 'tập_trung', 'sự', 'người', 'đa', 'công_cụ', 'thu_hút', 'xây_dựng', 'hình_ảnh', 'giáo_viên'}\n",
      "{'Ứng_dụng', 'dạy_học', 'công_nghệ_thông_tin'}\n",
      "{'âm_thanh', 'được', 'bài', 'văn_bản', 'đồ', 'học', 'máy_tính', 'giảng', 'phương_tiện', 'họa', 'tập_trung', 'sự', 'người', 'đa', 'công_cụ', 'thu_hút', 'xây_dựng', 'hình_ảnh', 'giáo_viên'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5020064592761162"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1='Nhờ các công cụ đa phương tiện của máy tính như văn bản, đồ họa, hình ảnh, âm thanh, giáo viên sẽ xây dựng được bài giảng sinh động thu hút sự tập trung của người học'\n",
    "doc2='Ứng dụng công nghệ thông tin vào dạy học.'\n",
    "\n",
    "is_document_related(doc1, doc2, word_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gibberish_detector import detector\n",
    "\n",
    "# open('data/WIKI.txt')\n",
    "\n",
    "Detector = detector.create_from_model('vie-model/gibberish.vie.model')\n",
    "\n",
    "def is_gibberish(doc):\n",
    "  return Detector.is_gibberish(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamFiltering:\n",
    "  def __init__(self):\n",
    "    self.detector = detector.create_from_model('vie-model/gibberish.vie.model')\n",
    "\n",
    "  def is_idea_spam(self, idea_object):\n",
    "    solution = idea_object.solution\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a704de84d788518585a1734d7e07d73c8af2617ee4bc5ea96944d1c9b2e73160"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
